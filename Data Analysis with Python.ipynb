{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a CSV without a header\n",
    "\n",
    "import pandas as pd\n",
    "url=\"LINK\"\n",
    "df=p.read_csv(url, header=none)\n",
    "\n",
    "#Printing the data frame in Python\n",
    "\n",
    "df # prints the entire dataframe (not recommended for large datasets)\n",
    "df.head(n) #to show the first n rows of the data frame\n",
    "df.tail(n) #shows the bottom n rows of data frame\n",
    "df.head()\n",
    "\n",
    "# Adding headers\n",
    "# Replace deafault headers (by df.columns=headers)\n",
    "\n",
    "headers=[\"\",\"\",\"etc\"]\n",
    "df.columns=headers\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to different formats in Python\n",
    "\n",
    "Data Format | Read | Save  \n",
    ":---: | :---: | :---: |  \n",
    "csv | pd.read_csv() | df.tp_csv()  \n",
    "json | pd.read_json() | df.to_json()  \n",
    "excel | pd.read_excel() | df.to_excel()  \n",
    "sql | pd.read_sql() | df.to_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pandas to check data types\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "# Returns a statistical summary\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# Provide full summary statistics\n",
    "\n",
    "df.describe(include=\"all\")\n",
    "\n",
    "# Now, the outcome shows the summary, including: object typed, unique, top, & frequency\n",
    "# Unique is the number of distinct objects in the column\n",
    "# Top is most frequently occurring object\n",
    "# Freq is the number of times the top object appears in the column\n",
    "# Some values in the table are shown here as NaN which stands for \"Not a Number\"\n",
    "\n",
    "df.info()\n",
    "\n",
    "# shows the top 30 rows and bottom 30 rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values along the column \"price\"\n",
    "\n",
    "df.dropna(subset=[\"price\"], axis=0)\n",
    "\n",
    "# You can select the columns of a data frame by indicating the name of  each column\n",
    "\n",
    "dataframe[['column 1','column 2','column 3']]\n",
    "\n",
    "# you can apply the method  \".describe()\" to get the statistics of those columns as follows:\n",
    "\n",
    "dataframe[['column 1','column 2','column 3']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data preprocessing is often called data cleaning or data wrangling\n",
    "\n",
    "# How to drop missing values in Python\n",
    "\n",
    "dataframes.dropna()\n",
    "\n",
    "# axis=0 drops the entire row\n",
    "# axis=1 drops the entire column\n",
    "\n",
    "df.dropna(subset=[\"price\"], axis=0, inplace=True)\n",
    "# Setting the argument in place to true, allows the modification to be done on the data set directly\n",
    "\n",
    "# How to replace missing values in Python\n",
    "\n",
    "dataframe.replace(missing_value,new_value)\n",
    "\n",
    "# replace it with an average\n",
    "\n",
    "mean=df[\"normalized-losses\"].mean()\n",
    "df[\"normalized-losses\"].replace(np.nan,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Formatting in Python\n",
    "\n",
    "df[\"city-mpg\"]=235/df[\"city-mpg\"]\n",
    "df.rename(columns={\"city_mpg\":\"city-L/100km\"},inplace=True)\n",
    "\n",
    "# Correcting data types\n",
    "\n",
    "# To identify data types:\n",
    "\n",
    "dataframe.dtypes()\n",
    "\n",
    "# To convert data types:\n",
    "\n",
    "dataframe.astype()\n",
    "\n",
    "# Example: convert data type to integer in column \"price\"\n",
    "\n",
    "df[\"price\"]=df[\"price\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of normalizing data\n",
    "\n",
    "###### length es el nombre de la variable/columna que se desea normalizar\n",
    "\n",
    "###### simple feature scaling just divides each value by the maximum value for that feature. This makes the new values range between zero and one.\n",
    "\n",
    "df[\"length\"]=df[\"length\"]/df[\"length\"].max()\n",
    "\n",
    "###### min-max takes each value X_old subtract it from the minimum value of that feature, then divides by the range of that feature. Again, the resulting new values range between zero and one.\n",
    "\n",
    "df[\"length\"]=(df[\"length\"]-df[\"length\"].min())/(df[\"length\"].max()-df[\"length\"].min())\n",
    "\n",
    "###### z-score or standard score. In this formula for each value you subtract the mu which is the average of the feature, and then divide by the standard deviation sigma. The resulting values hover around zero, and typically range between negative three and positive three but can be higher or lower.\n",
    "\n",
    "df[\"length\"]=(df[\"length\"]-df[\"length\"].mean())/df[\"length\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins = range by groups\n",
    "\n",
    "bins=np.linspace(min(df[\"price\"]),mad(df[\"price\"]),4)\n",
    "\n",
    "group_names=[\"Low\",\"Medium\",\"High\"]\n",
    "\n",
    "df[\"price-binned\"]=pd.cut(df[\"price\"],bins,labels=group_names,include_lowest=true)\n",
    "\n",
    "# Dummy variables in Python pandas also called one-hot encoding\n",
    "\n",
    "pd.get_dummies(df[\"fuel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How to work with missing data?</b>\n",
    "\n",
    "Steps for working with missing data:\n",
    "<ol>\n",
    "    <li>dentify missing data</li>\n",
    "    <li>deal with missing data</li>\n",
    "    <li>correct data format</li>\n",
    "</ol>\n",
    "\n",
    "<h2 id=\"identify_handle_missing_values\">Identify and handle missing values</h2>\n",
    "\n",
    "\n",
    "<h3 id=\"identify_missing_values\">Identify missing values</h3>\n",
    "<h4>Convert \"?\" to NaN</h4>\n",
    "In the car dataset, missing data comes with the question mark \"?\".\n",
    "We replace \"?\" with NaN (Not a Number), which is Python's default missing value marker, for reasons of computational speed and convenience. Here we use the function: \n",
    " <pre>.replace(A, B, inplace = True) </pre>\n",
    "to replace A by B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# replace \"?\" to NaN\n",
    "df.replace(\"?\", np.nan, inplace = True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True = missing data\n",
    "\n",
    "missing_data = df.isnull()\n",
    "missing_data.head(5)\n",
    "\n",
    "# Count missing data\n",
    "\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"deal_missing_values\">Deal with missing data</h3>\n",
    "<b>How to deal with missing data?</b>\n",
    "\n",
    "<ol>\n",
    "    <li>drop data<br>\n",
    "        a. drop the whole row<br>\n",
    "        b. drop the whole column\n",
    "    </li>\n",
    "    <li>replace data<br>\n",
    "        a. replace it by mean<br>\n",
    "        b. replace it by frequency<br>\n",
    "        c. replace it based on other functions\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "# summerize the categorical data is by using the value_counts() method\n",
    "\n",
    "drive_wheels_counts=df['drive-wheels'].value_counts()\n",
    "\n",
    "drive_wheels_counts.rename(columns={'drive-wheels':'value_counts'},inplace=True)\n",
    "drive_wheels_counts.index.name='drive-wheels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots:\n",
    "- The main features that the box plot shows are the median of the data, which represents where the middle data point is.\n",
    "- The upper quartile shows where the 75th percentile is.\n",
    "- The lower quartile shows where the 25th percentile is.\n",
    "- The data between the upper and lower quartile represents the interquartile range.\n",
    "- Next, you have the lower and upper extremes. These are calculated as 1.5 times the interquartile range above the 75th percentile and as 1.5 times the IQR below the 25th percentile.\n",
    "- Finally, box plots also display outliers as individual dots that occur outside the upper and lower extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The **predictor/independent** variable is the variable that you are using to predict an outcome.\n",
    "2. The **target/dependent** variable is the variable that you are trying to predict.\n",
    "3. In a scatter plot, we typically set the **predictor** variable on the **x-axis** or **horizontal axis**, and we set the **target** variable on the **y-axis** or **vertical axis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot example\n",
    "\n",
    "y=df['engine-size']\n",
    "x=df['price']\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.title('Scatterplot of Engine Size vs Price')\n",
    "plt.ylabel('Engine Size')\n",
    "plt.xlabel('Price')\n",
    "\n",
    "# GroupBy() example\n",
    "\n",
    "df_test=df['drive-wheels','body-style','price']\n",
    "df_grp=df_test.groupby(['drive-wheels','body-style'],as_index=False).mean()\n",
    "df_grp\n",
    "\n",
    "# Pivot Table\n",
    "\n",
    "df_pivot=df_grp.pivot(index='drive-wheels',columns='body-style')\n",
    "\n",
    "# Heatmap: plot target variable over multiple variables\n",
    "\n",
    "plt.color(df_pivot,cmap='RdBBu')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot correlation\n",
    "\n",
    "sns.regplot(x='engin-size',y='prices',data=df)\n",
    "plt.ylim(0,)\n",
    "\n",
    "# Pearson correlation example\n",
    "\n",
    "pearson_coef,p_value=stats.personr[['hoursepower'],df['price']]\n",
    "\n",
    "# ANOVA betwenn Honda & Subaru\n",
    "\n",
    "df_anova=df[['make','price']]\n",
    "grouped_anova=df_anova.groupby(['make'])\n",
    "\n",
    "F=0.19\n",
    "pvalue=0.66\n",
    "\n",
    "# ANOVA betwenn Honda & Jaguar\n",
    "\n",
    "F=400.92\n",
    "pvalue=1.05e-11\n",
    "\n",
    "# we can say that there's a strong correlation between a categorical variable and other variables\n",
    "# if the ANOVA test gives us a large F-test value and a small p-value\n",
    "\n",
    "# Fitting a Simple Linear Model Estimator\n",
    "\n",
    "X: Predictor variable\n",
    "y: Target variable\n",
    "\n",
    "# Import linear_model from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a Linear Regression Object using the constructor\n",
    "lm=LinearRegression()\n",
    "\n",
    "# Fitting a Simple Linear Model\n",
    "# We define the predictor variable and the target variable\n",
    "x=df[['highway-mpg']]\n",
    "y=df['price']\n",
    "\n",
    "# Then fit the model\n",
    "lm.fit(x,y)\n",
    "\n",
    "#We can obtain a prediction\n",
    "yhat=lm.predict(x)\n",
    "\n",
    "# Fitting a Multiple Linear Model Estimator\n",
    "\n",
    "# We can extract the 4 predictor variables and store them in the variable Z\n",
    "z=df[['horsepower','curb-weight','engine-size','highway-mpg']]\n",
    "\n",
    "# Then train the model as before\n",
    "lm.fit(z,df['price'])\n",
    "\n",
    "# Obtain the prediction\n",
    "yhat=lm.predict(x)\n",
    "\n",
    "# Find the intercept (b0)\n",
    "lm.intercept_\n",
    "\n",
    "# Find the coeficients (b1,b2,b3,b4)\n",
    "lm.coef_\n",
    "\n",
    "# The Estimated Linear Model\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + b3x3 + b4x4\n",
    "\n",
    "Price = -15678.74 + (52.66)*horsepower + (4.70)*curb-weight + (81.96)*engine_size + (33.58)*highway-mpg\n",
    "\n",
    "# Regression Plot\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x='highway-mpg',y='price',data=df)\n",
    "plt.ylim(0,)\n",
    "\n",
    "# Residual Plot\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.residplot(df['highway-mpg'],df['price'])\n",
    "\n",
    "# Distribution Plots\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "axl=sns.distplot(df['price'],hist=False,color='r',label=\"Actual Value\")\n",
    "\n",
    "sns.distplot(Yhat,hist=False,color=\"b\",label=\"Fitted Values\",ax=axl)\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "# Quadratic - 2ยบ order (Degree 2)\n",
    "y = b01 + b1x1 + b2(x1)^2\n",
    "\n",
    "#Cubic - 3ยบ order (Degree 3)\n",
    "y = b01 + b1x1 + b2(x1)^2 + b3(x1)^3\n",
    "\n",
    "# Higher order (more Degrees)\n",
    "y = b01 + b1x1 + b2(x1)^2 + b3(x1)^3 + ...\n",
    "\n",
    "#Calculate Polynomial of 3ยบ order\n",
    "\n",
    "f=np.polyfit(x,y,3)\n",
    "p=np.polyfit(f)\n",
    "print(p)\n",
    "\n",
    "# Polynomial Regression with more than one dimension\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pr=PolynomialFeatures(degree=2)\n",
    "x_polly=pr.fit_transform(x['horsepower','curb-weight'],include_bias=False)\n",
    "\n",
    "# Pre-processing\n",
    "# We can Normalize the each feature simultaneously\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SCALE=StandardScaler()\n",
    "SCALE.fit(x_data[['horsepower','highway-mpg']])\n",
    "x_scale=SCALE.transform(x_data[['horsepower','highway-mpg']])\n",
    "\n",
    "# Pipelines\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandarScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline Constructor\n",
    "\n",
    "Input=[('scale',StandarScaler()),('polynomial',PolynomialFeatures(degree=2),('mode',LinearRegression())]\n",
    "pipe=Pipeline(Input)\n",
    "\n",
    "# We can train the pipeline object\n",
    "\n",
    "Pipe.train(X['horsepower','curb-weight','engine-size','highway-mpg'],y)\n",
    "yhat=Pipe.predict(X[['horsepower','curb-weight','engine-size','highway-mpg']])\n",
    "\n",
    "# Measures for In-Sample Evaluation\n",
    "\n",
    "# Mean Square Error (MSE)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(df['price'],Y_predict_simple_fit)\n",
    "\n",
    "# R-squared\n",
    "\n",
    "x=df[['highway-mpg']]\n",
    "y=df['price']\n",
    "\n",
    "lm.fit(x,y)\n",
    "lm.score(x,y)\n",
    "\n",
    "# we train the model\n",
    "\n",
    "lm.fit(df['highway-mpg'],df['prices'])\n",
    "\n",
    "# Predict the price of a car with 30 highway-mpg\n",
    "\n",
    "lm.predict(30)\n",
    "\n",
    "# We use the numpy function arrange to generate a sequence from 1 to 100\n",
    "\n",
    "import numpy as np\n",
    "new_input=np.arange(1,101,1).reshape(-1,1)\n",
    "\n",
    "# The first parameter is the starting point of the sequence = 1\n",
    "# The second parameter is the endpoint plus one of the sequence = 101\n",
    "# The final parameter is the step size between elements in the sequence = 1\n",
    "\n",
    "# We can predict new values\n",
    "\n",
    "yhat=lm.predict(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Refinement\n",
    "\n",
    "# Split data into random train and test subsets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.3,random_state=0)\n",
    "\n",
    "# Donde\n",
    "# x_data: features or independent variables\n",
    "# y_data: dataset target: df['price']\n",
    "# x_train & y_train: parts of available data as training set\n",
    "# y_test & y_test: parts of available data as testing set\n",
    "# test_size: percentage of the data for testing (here 30%)\n",
    "\n",
    "# Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores=cross_val_score(lr,x_data,y_data,cv=3)\n",
    "\n",
    "np.mean(scores)\n",
    "\n",
    "# The first input parameters:\n",
    "# the type of model we are using to do the cross-validation: In this example, we initialize a linear regression model or lr\n",
    "# x_data: the predictive variable data\n",
    "# y_data: the target variable data\n",
    "# cv: the number of partitions with the cv parameter. Here, cv=3, which means the data set is split into 3 equal partitions\n",
    "# The function returns an array of scores, one for each partition that was chosen as the testing set\n",
    "# We can average the result together to estimate out of sample r squared using the mean function NnumPi.\n",
    "\n",
    "# Cross Validation Prediction\n",
    "# it returns the prediction that was obtained for each element when it was in the test set\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "yhat=cross_val_predict(lr2e,x_data,y_data,cv=3)\n",
    "\n",
    "# Overfitting, Underfitting and Model Selection\n",
    "\n",
    "Rsqu_test=[]\n",
    "order=[1,2,3,4]\n",
    "for n in order:\n",
    "pr=PolynomialFeatures(degree=n)\n",
    "x_train_pr=pr.fit_transform(x_train[['horsepower']])\n",
    "x_test_pr=pr.fit_transform(x_test[['horsepower']])\n",
    "lr.fit(x_train_pr,y_train)\n",
    "Rsqu_test.append(lr.score(x_test_pr,y_test))\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "RidgeModel=Ridge(alpha=0.1)\n",
    "RodgeModel.fit(x,y)\n",
    "yhat=RidgeModel.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As alpha increases the parameters get smaller*\\\n",
    "This is most evident for the higher order polynomial features. But Alpha must be selected carefully.\\\n",
    "If alpha is too large, the coefficients will approach zero and **underfit** the data.\\\n",
    "If alpha is zero, the **overfitting** is evident.\n",
    "\n",
    "Example:\n",
    "- Alpha = 0.001, the overfitting begins to subside.\n",
    "- Alpha = 0.01, the estimated function tracks the actual function.\n",
    "- Alpha = 1, we see the first signs of **underfitting**\n",
    "- Alpha = 10, we see extreme **underfitting**\n",
    "\n",
    "*Conversely, as alpha increases, the R-squared on the test data decreases.\n",
    "This is because the term Alpha prevents overfitting.\n",
    "This may improve the results in the unseen data, but the model has worse performance on the test data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "The term Alpha in Ridge regression is called hyperparameter\n",
    "Scikit-learn has a means of automatically iterating over these hyperparameters using cross-validation called: **Grid Search**\n",
    "\n",
    "*What data do we use to pick the best hyperparameter?*\n",
    "Response: Validation Data\n",
    "\n",
    "The value of your Grid Search is a Python list that contains a Python dictionary.\n",
    "The key is the name of the free parameter ('alpha').\n",
    "The value of the dictionary is the different values of the free parameter (1,10,100,1000).\n",
    "\n",
    "parameters=[{'alpha':[1,10,100,1000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters1=[{'alpha':[0.001,0.1,1,10,100,1000,10000,100000,100000]}]\n",
    "RR=Ridge()\n",
    "Grid1=GridSearchCV(RR,parameters1,cv=4)\n",
    "Grid1.fit(x_data[['horsepower','curb-weight','engine-size','highway-mpg']],y_data)\n",
    "Grid1.best_estimator_\n",
    "scores=Grid1.cv_results_\n",
    "scores['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression to normalize data\n",
    "\n",
    "parameters=[{'alpha':[1,10,100,1000],'normalize':[True,False]}]\n",
    "Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search with normalized data\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters2=[{'alpha':[1,10,100,1000],'normalize':[True,False]}]\n",
    "RR=Ridge()\n",
    "Grid1=GridSearchCV(RR,parameters2,cv=4)\n",
    "Grid1.fit(x_data[['horsepower','curb-weight','engine-size','highway-mpg']],y_data)\n",
    "Grid1.best_estimator_\n",
    "scores=Grid1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print out the score for the different free parameter values\n",
    "\n",
    "for param,mean_val,mean_test inzip(scores['params'],scores['mean_test_score'],scores['mean_train_score']):\n",
    "print(param,'R^2 on test data:', mean_val,'R^2 on train data:', mean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
